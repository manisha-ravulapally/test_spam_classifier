# -*- coding: utf-8 -*-
"""sms.spam.classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rf_rLZwn9kR9utfcMxY0E6q62vOFF8Iu

# **DATASET:** **SMS Spam Classifier.**
# **FROM:** **KAGGLE** *https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download*
"""

import numpy as np
import pandas as pd

"""

1. ** LOADING THE CSV FILE**
2.   **encoding = "ISO-8859-1" ---- is used enncodes most foreign languages.**

"""

df = pd.read_csv('spam.csv',encoding = "ISO-8859-1")

df.head()

df.shape

"""# **1.DATA CLEANING**"""

df.info()

df.isnull().sum()

"""*This dataset has columns called Unnamed: 2 ,Unnamed: 3,Unnamed: 4  which are of no use .. so just remove that columns using drop()*"""

df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

df.head()

"""*rename the v1 & v2 columns as **"target" & "text"** for better understanding*"""

df.rename(columns={'v1':'target','v2':'text'},inplace=True)

df.head()

"""**LabelEncoder**--->*TO TRANSFORM TARGET COLUMN WHICH IS IN THE CATEGORICAL FORM TO NUMERICAL VALUES*"""

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target'] = encoder.fit_transform(df['target'])

df.head()

"""**1---> SPAM**

**0---> HAM**

**Checking for null values**
"""

df.isnull().sum()

"""**checking duplicates**"""

df.duplicated().sum()

"""**droping the duplicates by considering the first value as unique and rest of the same values as duplicates.**

"""

df = df.drop_duplicates(keep='first')

df.duplicated().sum()

df.shape

"""**2.EDA**"""

df.head()

y=df.groupby(df['target']).count()
y

"""*Dataset has 4516 number of ham type and 653 number of records of spam type*"""

import matplotlib.pyplot as plt
plt.pie(y['text'], labels=['ham','spam'],autopct="%0.2f")
plt.show()

"""*Data is imbalanced*

**3.TEXT PREPROCESSING**

**Natural Language Toolkit:** *contains text processing libraries for tokenization,stemming, stopwords etc*

1.   **Tokenization:**The process of breaking down the text data into individual words, sentences, characters which are called as tokens.
2.   List item
"""

!pip install nltk

import nltk

nltk.download('punkt')

df['num_characters'] = df['text'].apply(len)

df.head()

"""*spliting data  into words --> words tokenization*"""

from nltk.tokenize import word_tokenize
text_to_words = word_tokenize(df['text'].iloc[0])
print(text_to_words)

df['num_words'] = df['text'].apply(lambda x:len(word_tokenize(x)))

df.head()

"""*spliting data  into sentences --> sentence tokenization*"""

from nltk.tokenize import sent_tokenize
text_to_sentence = sent_tokenize(df['text'].iloc[0])
print(text_to_sentence)

df['num_sentences'] = df['text'].apply(lambda x:len(sent_tokenize(x)))

df.head()

df[['num_characters','num_words','num_sentences']].describe()

"""**for ham type**"""

df[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()

"""**for spam type**"""

df[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()

import seaborn as sns

plt.figure(figsize=(12,6))
sns.histplot(df[df['target'] == 0]['num_characters'])
sns.histplot(df[df['target'] == 1]['num_characters'],color='red')

"""**Number of characters are more in the spam sms when compared to ham sms**"""

plt.figure(figsize=(12,6))
sns.histplot(df[df['target'] == 0]['num_words'])
sns.histplot(df[df['target'] == 1]['num_words'],color='red')

"""**Number of words are more in the spam sms when compared to ham sms**"""

sns.pairplot(df,hue='target')

sns.heatmap(df.corr(),annot=True)

"""**DATA PREPROCESSING**

**Stop words---> words which are repetitive and donâ€™t hold any information**
"""

from nltk.corpus import stopwords
nltk.download('stopwords')

text = 'Learn to lose your destiny to find where it leads you'
filtered_text = []
tokenized_word = word_tokenize(text)
for each_word in tokenized_word:
  if each_word not in  stopwords.words('english'):
    filtered_text.append(each_word)
print(text)
filtered_text

import string

"""**Stemming is a process in which words are reduced to their root word (or) stem**"""

from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()

"""

*  Lower case
* Removing special characters
* Removing stop words and punctuation
* Stemming



"""

def transform_text(text):
    text = text.lower()
    text = word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        y.append(ps.stem(i))


    return " ".join(y)

transform_text("I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today")

df['transformed_text'] = df['text'].apply(transform_text)

df.head()

"""**Understanding the words used in ham and spam sms and finding out the most used words respectively**"""

spam_ls = []
for msg in df[df['target'] == 1]['transformed_text'].tolist():
    for word in msg.split():
        spam_ls.append(word)
spam_ls

len(spam_ls)

from nltk.probability import FreqDist
spam_words = FreqDist(spam_ls)
print(spam_words)

spam_words.plot(30)
plt.show()

ham_ls= []
for msg in df[df['target'] == 0]['transformed_text'].tolist():
    for word in msg.split():
        ham_ls.append(word)

ham_words = FreqDist(ham_ls)
print(ham_words)

ham_words.plot(30)
plt.show()

"""**Conclusion:** *We have cleaned the raw data by removing null values,dropping unneccessary columns and duplicates.*

*We explored the data in order to find out the characteristics of spam and ham messages like typically which one of these two have more number of characters, words, sentences and what are the most used words respectively etc*

*Then we pre processed the text data. We converted the text into lower case, removed special charaters, stopwords, punctuations and stemmed the text*

**DATA TRANSFORMATION**
"""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()

X=cv.fit_transform(df['transformed_text']).toarray()

X

X.shape

y=df['target'].values

y

"""# *Train test split*"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

knc = KNeighborsClassifier()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression()
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)

clfs = {
    'KN' : knc,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'GBDT':gbdt
}

from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

def train_classifier(clf,X_train,y_train,X_test,y_test):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred)

    return accuracy,precision

accuracy_scores = []
precision_scores = []

for name,clf in clfs.items():

    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)



